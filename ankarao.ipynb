{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPqKgH5j5YVb/mYXcQHKBLB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":401},"id":"1Dktd039Ycnc","executionInfo":{"status":"error","timestamp":1761043755552,"user_tz":-330,"elapsed":62620,"user":{"displayName":"ankarao s","userId":"12707184833034441709"}},"outputId":"6735427f-aa58-4779-e394-8f69e11c287f"},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'controlnet_aux'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-457093553.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradio\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdiffusers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStableDiffusionInpaintPipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mControlNetModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStableDiffusionControlNetInpaintPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcontrolnet_aux\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOpenposeDetector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'controlnet_aux'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["\"\"\"\n","AI Fashion Model Visualization Tool – MVP reference implementation\n","Author : You\n","Date   : 21-Oct-2025\n","Hardware: NVIDIA GPU ≥8 GB VRAM (RTX 3060 / T4)\n","Runtime : ±5 s / image @ 512×512\n","\"\"\"\n","\n","# ------------------ 1. One-shot install ------------------\n","# !pip install -q diffusers==0.30.0 transformers accelerate xformers opencv-python controlnet_aux safetensors torch --upgrade\n","\n","# ------------------ 2. Imports ------------------\n","import torch, cv2, numpy as np, os, gradio as gr\n","from diffusers import StableDiffusionInpaintPipeline, ControlNetModel, StableDiffusionControlNetInpaintPipeline\n","from controlnet_aux import OpenposeDetector\n","from PIL import Image\n","import warnings, gc, time\n","warnings.filterwarnings(\"ignore\")\n","\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","DTYPE  = torch.float16 if DEVICE==\"cuda\" else torch.float32\n","\n","# ------------------ 3. Download weights (cached) ------------------\n","base_id   = \"runwayml/stable-diffusion-inpainting\"   # public SD 1.5 inpainting\n","ctrl_id   = \"lllyasviel/control_v11p_sd15_openpose\"\n","lora_path = \"https://huggingface.co/your-org/fashion-lora/resolve/main/fashion-lora.safetensors\"  # <-- replace with your LoRA\n","\n","# ------------------ 4. Load models once ------------------\n","print(\"Loading models …\")\n","controlnet = ControlNetModel.from_pretrained(ctrl_id, torch_dtype=DTYPE).to(DEVICE)\n","pipe = StableDiffusionControlNetInpaintPipeline.from_pretrained(\n","        base_id, controlnet=controlnet, torch_dtype=DTYPE, safety_checker=None)\n","pipe.enable_xformers_memory_efficient_attention()\n","pipe.enable_model_cpu_offload()          # low VRAM friendly\n","if DEVICE==\"cuda\": pipe.enable_attention_slicing(1)\n","\n","# Optional – load LoRA ( CivitAI or your own )\n","# pipe.load_lora_weights(lora_path, adapter_name=\"fashion\")\n","# pipe.set_adapters([\"fashion\"], adapter_weights=[1.0])\n","\n","openpose = OpenposeDetector.from_pretrained(\"lllyasviel/Annotators\")\n","\n","# ------------------ 5. Core try-on logic ------------------\n","def auto_mask(garment_flat:Image)->Image:\n","    \"\"\"Very crude garment mask via HSV saturation – replace with U²-Net later.\"\"\"\n","    im = np.array(garment_flat)\n","    hsv = cv2.cvtColor(im, cv2.COLOR_RGB2HSV)\n","    mask = cv2.inRange(hsv, (0,50,50), (180,255,255))\n","    mask = cv2.GaussianBlur(mask, (11,11), 0)\n","    return Image.fromarray(mask)\n","\n","def try_on(garment_path:str, person_path:str, guidance=7.5, steps=25, seed=42):\n","    start = time.time()\n","    garment = Image.open(garment_path).convert(\"RGB\").resize((512,512))\n","    person  = Image.open(person_path).convert(\"RGB\").resize((512,512))\n","    mask    = auto_mask(garment).resize((512,512))\n","\n","    pose_img = openpose(person).resize((512,512))\n","\n","    generator = torch.Generator(DEVICE).manual_seed(seed)\n","    result = pipe(\n","        prompt=\"fashion model wearing the garment, high quality, detailed fabric\",\n","        negative_prompt=\"blurry, lowres, distorted face, extra limbs\",\n","        image=person,\n","        mask_image=mask,\n","        control_image=pose_img,\n","        num_inference_steps=steps,\n","        guidance_scale=guidance,\n","        generator=generator,\n","        width=512,\n","        height=512\n","    ).images[0]\n","\n","    print(f\"DONE in {time.time()-start:.1f}s\")\n","    return result\n","\n","# ------------------ 6. Gradio UI (optional) ------------------\n","demo = gr.Interface(\n","    fn=try_on,\n","    inputs=[gr.Image(type=\"filepath\", label=\"Flat-lay garment\"),\n","            gr.Image(type=\"filepath\", label=\"Customer photo\")],\n","    outputs=gr.Image(type=\"pil\", label=\"AI Try-On\"),\n","    title=\"AI Fashion Model Visualization Tool – MVP\",\n","    examples=[[\"./sample_garment.jpg\", \"./sample_person.jpg\"]]\n",")\n","\n","# ------------------ 7. CLI entry ------------------\n","if __name__ == \"__main__\":\n","    import fire\n","    fire.Fire(try_on)   # python ai_fashion_tryon.py garment.jpg person.jpg"]}]}